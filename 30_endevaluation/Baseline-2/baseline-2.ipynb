{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11125638,"sourceType":"datasetVersion","datasetId":6938319},{"sourceId":11275136,"sourceType":"datasetVersion","datasetId":7048730},{"sourceId":11381444,"sourceType":"datasetVersion","datasetId":7123796}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch sklearn tqdm indic-transliteration\n!pip install indic-transliteration\n!pip install --upgrade indic-transliteration","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:17.274084Z","iopub.execute_input":"2025-04-15T16:11:17.274592Z","iopub.status.idle":"2025-04-15T16:11:24.961985Z","shell.execute_reply.started":"2025-04-15T16:11:17.274567Z","shell.execute_reply":"2025-04-15T16:11:24.961264Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting sklearn\n  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nRequirement already satisfied: indic-transliteration in /usr/local/lib/python3.11/dist-packages (2.3.69)\nRequirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2.0.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2024.11.6)\nRequirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.15.1)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.10.2)\nRequirement already satisfied: roman in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (5.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (8.1.8)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (4.13.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (14.0.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\nRequirement already satisfied: indic-transliteration in /usr/local/lib/python3.11/dist-packages (2.3.69)\nRequirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2.0.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2024.11.6)\nRequirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.15.1)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.10.2)\nRequirement already satisfied: roman in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (5.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (8.1.8)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (4.13.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (14.0.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\nimport os\nfrom typing import Dict, Tuple, List\nfrom sklearn.model_selection import train_test_split\n\n# Disable wandb logging\nos.environ['WANDB_MODE'] = 'disabled'\n\n\ndef convert_hinglish_to_hindi(text: str) -> str:\n    \n    def is_mostly_roman(text: str) -> bool:\n        \n        if not text:\n            return False\n        devanagari_chars = sum(1 for ch in text if '\\u0900' <= ch <= '\\u097F')\n        return (devanagari_chars / len(text)) < 0.3\n        \n    def transliterate_to_hindi(text: str) -> str:\n        \"\"\"Convert Roman text to Devanagari.\"\"\"\n        from indic_transliteration import sanscript\n        return sanscript.transliterate(text, sanscript.ITRANS, sanscript.DEVANAGARI)\n\n    if not text:\n        return text\n    return transliterate_to_hindi(text) if is_mostly_roman(text) else text\n\ndef aggregate_labels(df: pd.DataFrame, lang: str) -> pd.DataFrame:\n    \n    def get_annotator_cols(lang: str) -> List[str]:\n        \n        return {\n            'en': [f\"en_a{i}\" for i in range(1, 7)],\n            'hi': [f\"hi_a{i}\" for i in range(1, 6)], \n            'ta': [f\"ta_a{i}\" for i in range(1, 7)],\n        }[lang]\n    \n    def calculate_majority_label(row: pd.Series) -> int:\n        \n        return int(row.mean() >= 0.5)\n\n    annotator_cols = get_annotator_cols(lang)\n    df[annotator_cols] = df[annotator_cols].apply(pd.to_numeric, errors='coerce')\n    df['label'] = df[annotator_cols].apply(calculate_majority_label, axis=1)\n    return df\n\n\nclass Vocabulary:\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n        \n        # Add special tokens\n        self.add_word('<pad>')  # Padding token\n        self.add_word('<unk>')  # Unknown word token\n        \n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n            \n    def __len__(self):\n        return len(self.word2idx)\n\ndef build_vocab(texts, min_freq=2):\n    def count_words(texts):\n        \n        word_counts = defaultdict(int)\n        for text in texts:\n            for word in text.split():\n                word_counts[word.lower()] += 1\n        return word_counts\n    \n    def add_frequent_words(vocab, word_counts, min_freq):\n        \n        for word, count in word_counts.items():\n            if count >= min_freq:\n                vocab.add_word(word)\n        return vocab\n    \n    vocab = Vocabulary()\n    word_counts = count_words(texts)\n    vocab = add_frequent_words(vocab, word_counts, min_freq)\n    return vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:24.963892Z","iopub.execute_input":"2025-04-15T16:11:24.964110Z","iopub.status.idle":"2025-04-15T16:11:24.979366Z","shell.execute_reply.started":"2025-04-15T16:11:24.964091Z","shell.execute_reply":"2025-04-15T16:11:24.978654Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, vocab, labels=None, label2=None):\n        self.texts = []\n        self.vocab = vocab\n        self.labels = []\n        self.label2 = []\n        \n        for i, text in enumerate(texts):\n            if isinstance(text, str) and text.strip():\n                self.texts.append(text)\n                if labels is not None and i < len(labels):\n                    self.labels.append(labels[i])\n                if label2 is not None and i < len(label2):\n                    self.label2.append(label2[i])\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        def get_text_indices(text):\n            \n            return [self.vocab.word2idx.get(word.lower(), self.vocab.word2idx['<unk>']) \n                    for word in text.split()]\n        \n        def build_item_dict(indices):\n             \n            item = {\n                'text': torch.tensor(indices, dtype=torch.long),\n            }\n            if len(self.labels) > 0:\n                item['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n            if len(self.label2) > 0:\n                item['label2'] = torch.tensor(self.label2[idx], dtype=torch.long)\n            return item\n        \n        text = self.texts[idx]\n        indices = get_text_indices(text)\n        return build_item_dict(indices)\n\ndef collate_fn(batch):\n    \n    def build_base_dict(texts):\n        \n        texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n        return {\n            'text': texts_padded,\n            'lengths': torch.tensor([len(text) for text in texts])\n        }\n        \n    def add_labels(batch_dict, batch): \n        \"\"\"Add label tensors if present in batch\"\"\"\n        if 'label' in batch[0]:\n            batch_dict['label'] = torch.tensor([item['label'] for item in batch])\n            \n        if 'label2' in batch[0]:\n            batch_dict['label2'] = torch.tensor([item['label2'] for item in batch])\n        return batch_dict\n        \n    texts = [item['text'] for item in batch]\n    batch_dict = build_base_dict(texts)\n    return add_labels(batch_dict, batch)\n\ndef load_data(lang: str, label_type: str, is_train: bool = True) -> pd.DataFrame:\n    \n\n    def read_csv_with_fallback(filename: str) -> pd.DataFrame:\n        \n        try:\n            # First try standard read with error_bad_lines=False\n            df = pd.read_csv(filename, on_bad_lines='warn', engine='python')\n        except Exception as e:\n            print(f\"Standard read failed for {filename}: {str(e)}\")\n            try:\n                # Try with Python engine\n                df = pd.read_csv(filename, engine='python', on_bad_lines='skip')\n            except Exception as e:\n                print(f\"Python engine read failed for {filename}: {str(e)}\")\n                try:\n                    # Try reading raw file with error handling\n                    with open(filename, 'r', encoding='utf-8') as f:\n                        lines = f.readlines()\n                    # Simple CSV parsing if standard methods fail\n                    data = [line.strip().split(',') for line in lines]\n                    df = pd.DataFrame(data[1:], columns=data[0])\n                except Exception as e:\n                    print(f\"All read methods failed for {filename}: {str(e)}\")\n                    return pd.DataFrame()\n        return df\n\n    def process_dataframe(df: pd.DataFrame, lang: str) -> pd.DataFrame:\n        \n        if len(df) == 0:\n            return df\n        \n        if lang == 'hi':\n            df['text'] = df['text'].apply(convert_hinglish_to_hindi)\n        return df\n\n    def ensure_required_columns(df: pd.DataFrame, lang: str) -> pd.DataFrame:\n        \n        required_cols = ['text'] + [f\"{lang}_a{i}\" for i in range(1, 7 if lang in ['en', 'ta'] else 6)]\n        for col in required_cols:\n            if col not in df.columns:\n                df[col] = 0  # Default value if column is missing\n        return df\n\n    # Main execution flow\n    filename = f\"/kaggle/input/gender-abuse-{'train' if is_train else 'test'}/{'train' if is_train else 'test'}_{lang}_{label_type}.csv\"\n    df = read_csv_with_fallback(filename)\n    df = process_dataframe(df, lang)\n    df = ensure_required_columns(df, lang)\n    return aggregate_labels(df, lang)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:24.980153Z","iopub.execute_input":"2025-04-15T16:11:24.980390Z","iopub.status.idle":"2025-04-15T16:11:25.000615Z","shell.execute_reply.started":"2025-04-15T16:11:24.980371Z","shell.execute_reply":"2025-04-15T16:11:25.000138Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class BiLSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n                 n_layers, dropout, pretrained_embeddings=None):\n        super().__init__()\n        \n        def init_embedding():\n            \n            if pretrained_embeddings is not None:\n                return nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n            return nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        \n        def init_layers():\n            \n            lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n                          bidirectional=True, dropout=dropout, batch_first=True)\n            fc = nn.Linear(hidden_dim * 2, output_dim)  \n            dropout_layer = nn.Dropout(dropout)\n            return lstm, fc, dropout_layer\n        \n        \n        self.embedding = init_embedding()\n        self.lstm, self.fc, self.dropout = init_layers()\n        \n    def forward(self, text, text_lengths):\n        def get_lstm_output(embedded):\n            \n            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n                embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n            return self.lstm(packed_embedded)\n        \n        def process_hidden_state(hidden):\n            \n            combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n            return self.dropout(combined)\n        \n        # Main forward pass\n        embedded = self.dropout(self.embedding(text))\n        packed_output, (hidden, cell) = get_lstm_output(embedded)\n        hidden_processed = process_hidden_state(hidden)\n        \n        return self.fc(hidden_processed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.002044Z","iopub.execute_input":"2025-04-15T16:11:25.002625Z","iopub.status.idle":"2025-04-15T16:11:25.017987Z","shell.execute_reply.started":"2025-04-15T16:11:25.002609Z","shell.execute_reply":"2025-04-15T16:11:25.017462Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class MultiTaskBiLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n                     n_layers, dropout, pretrained_embeddings=None):\n            super().__init__()\n            \n            def init_embedding():\n                \n                if pretrained_embeddings is not None:\n                    return nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n                return nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n            \n            def init_layers():\n                \n                lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n                              bidirectional=True, dropout=dropout, batch_first=True)\n                fc1 = nn.Linear(hidden_dim * 2, output_dim)  # For task 1\n                fc2 = nn.Linear(hidden_dim * 2, output_dim)  # For task 2\n                dropout_layer = nn.Dropout(dropout)\n                return lstm, fc1, fc2, dropout_layer\n            \n            # Initialize components using nested functions\n            self.embedding = init_embedding()\n            self.lstm, self.fc1, self.fc2, self.dropout = init_layers()\n        \n    def forward(self, text, text_lengths):\n        def process_shared_layers():\n            \n            embedded = self.dropout(self.embedding(text))\n            packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n            packed_output, (hidden, cell) = self.lstm(packed_embedded)\n            return self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n            \n        def get_task_outputs(hidden):\n            \n            output1 = self.fc1(hidden)\n            output2 = self.fc2(hidden)\n            return output1, output2\n\n        # Main forward pass        \n        hidden = process_shared_layers()\n        return get_task_outputs(hidden)\n\n# ======== 3. Training Functions ============\ndef train_single_task(model, train_loader, val_loader, epochs=5, learning_rate=1e-3):\n    def setup_training():\n        \n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model.to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        criterion = nn.CrossEntropyLoss()\n        best_f1 = 0\n        history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n        return device, optimizer, criterion, best_f1, history\n\n    def train_epoch(optimizer, criterion, device):\n        \n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n            optimizer.zero_grad()\n            texts = batch['text'].to(device)\n            lengths = batch['lengths'].to(device)\n            labels = batch['label'].to(device)\n            predictions = model(texts, lengths)\n            loss = criterion(predictions, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        return total_loss / len(train_loader)\n\n    def validate_model(device):\n        \n        val_f1, val_loss = evaluate_single_task(model, val_loader, device)\n        return val_f1, val_loss\n\n    def update_history(train_loss, val_loss, val_f1, history, best_f1):\n        \n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_f1'].append(val_f1)\n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n        return best_f1\n\n    # Main training loop\n    device, optimizer, criterion, best_f1, history = setup_training()\n    for epoch in range(epochs):\n        train_loss = train_epoch(optimizer, criterion, device)\n        val_f1, val_loss = validate_model(device)\n        best_f1 = update_history(train_loss, val_loss, val_f1, history, best_f1)\n    \n    model.load_state_dict(torch.load('best_model.pt'))\n    return model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.018873Z","iopub.execute_input":"2025-04-15T16:11:25.019114Z","iopub.status.idle":"2025-04-15T16:11:25.034649Z","shell.execute_reply.started":"2025-04-15T16:11:25.019094Z","shell.execute_reply":"2025-04-15T16:11:25.034054Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def evaluate_single_task(model, data_loader, device):\n    def setup_evaluation():\n        \n        model.eval()\n        return [], [], 0, nn.CrossEntropyLoss()\n\n    def process_batch(batch, preds, true_labels, total_loss):\n        \n        texts = batch['text'].to(device)\n        lengths = batch['lengths'].to(device)\n        labels = batch['label'].to(device)\n        \n        predictions = model(texts, lengths)\n        loss = criterion(predictions, labels)\n        batch_preds = torch.argmax(predictions, dim=-1)\n        \n        preds.extend(batch_preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n        return loss.item()\n        \n    def calculate_metrics(total_loss, preds, true_labels):\n        \"\"\"Calculate and return final metrics\"\"\"\n        avg_loss = total_loss / len(data_loader)\n        f1 = f1_score(true_labels, preds, average='macro')\n        return f1, avg_loss\n\n    # Main evaluation flow\n    preds, true_labels, total_loss, criterion = setup_evaluation()\n    with torch.no_grad():\n        for batch in data_loader:\n            total_loss += process_batch(batch, preds, true_labels, total_loss)\n    \n    return calculate_metrics(total_loss, preds, true_labels)\n\ndef train_multi_task(model, train_loader, val_loader, epochs=5, learning_rate=1e-3):\n    def setup_training():\n        \n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model.to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        criterion = nn.CrossEntropyLoss()\n        best_avg_f1 = 0\n        history = {'train_loss': [], 'val_loss': [], 'val_f1_l1': [], 'val_f1_l3': []}\n        return device, optimizer, criterion, best_avg_f1, history\n\n    def train_epoch(optimizer, criterion, device):\n        \n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n            optimizer.zero_grad()\n            texts, lengths = batch['text'].to(device), batch['lengths'].to(device)\n            labels1, labels2 = batch['label'].to(device), batch['label2'].to(device)\n            \n            predictions1, predictions2 = model(texts, lengths)\n            loss = criterion(predictions1, labels1) + criterion(predictions2, labels2)\n            \n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        return total_loss / len(train_loader)\n\n    def validate_model(device):\n        \n        (f1_l1, val_loss1), (f1_l3, val_loss2) = evaluate_multi_task(model, val_loader, device)\n        avg_val_loss = (val_loss1 + val_loss2) / 2\n        avg_f1 = (f1_l1 + f1_l3) / 2\n        return f1_l1, f1_l3, avg_val_loss, avg_f1\n\n    def update_history(train_loss, val_metrics, history, best_avg_f1):\n        \n        f1_l1, f1_l3, avg_val_loss, avg_f1 = val_metrics\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(avg_val_loss)\n        history['val_f1_l1'].append(f1_l1)\n        history['val_f1_l3'].append(f1_l3)\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n        print(f\"L1: F1={f1_l1:.4f} | L3: F1={f1_l3:.4f}\")\n        \n        if avg_f1 > best_avg_f1:\n            best_avg_f1 = avg_f1\n            torch.save(model.state_dict(), 'best_multi_model.pt')\n        return best_avg_f1\n\n    # Main training loop\n    device, optimizer, criterion, best_avg_f1, history = setup_training()\n    for epoch in range(epochs):\n        train_loss = train_epoch(optimizer, criterion, device)\n        val_metrics = validate_model(device)\n        best_avg_f1 = update_history(train_loss, val_metrics, history, best_avg_f1)\n    \n    model.load_state_dict(torch.load('best_multi_model.pt'))\n    return model, history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.035508Z","iopub.execute_input":"2025-04-15T16:11:25.035727Z","iopub.status.idle":"2025-04-15T16:11:25.051900Z","shell.execute_reply.started":"2025-04-15T16:11:25.035712Z","shell.execute_reply":"2025-04-15T16:11:25.051379Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def evaluate_multi_task(model, data_loader, device):\n    def process_batch(batch, criterion):\n        \n        texts = batch['text'].to(device)\n        lengths = batch['lengths'].to(device)\n        labels1 = batch['label'].to(device)\n        labels2 = batch['label2'].to(device)\n        \n        predictions1, predictions2 = model(texts, lengths)\n        loss1 = criterion(predictions1, labels1)\n        loss2 = criterion(predictions2, labels2)\n        \n        batch_preds1 = torch.argmax(predictions1, dim=-1)\n        batch_preds2 = torch.argmax(predictions2, dim=-1)\n        \n        return (\n            batch_preds1, batch_preds2, labels1, labels2,\n            loss1.item(), loss2.item()\n        )\n    \n    def calculate_metrics(preds_l1, preds_l3, true_l1, true_l3, total_loss1, total_loss2):\n        \n        avg_loss1 = total_loss1 / len(data_loader)\n        avg_loss2 = total_loss2 / len(data_loader)\n        f1_l1 = f1_score(true_l1, preds_l3, average='macro')\n        f1_l3 = f1_score(true_l3, preds_l3, average='macro')\n        return (f1_l1, avg_loss1), (f1_l3, avg_loss2)\n    \n    model.eval()\n    preds_l1, preds_l3, true_l1, true_l3 = [], [], [], []\n    total_loss1, total_loss2 = 0, 0\n    criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            batch_preds1, batch_preds2, labels1, labels2, loss1, loss2 = process_batch(batch, criterion)\n            preds_l1.extend(batch_preds1.cpu().numpy())\n            preds_l3.extend(batch_preds2.cpu().numpy())\n            true_l1.extend(labels1.cpu().numpy())\n            true_l3.extend(labels2.cpu().numpy())\n            total_loss1 += loss1\n            total_loss2 += loss2\n    \n    return calculate_metrics(preds_l1, preds_l3, true_l1, true_l3, total_loss1, total_loss2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.052674Z","iopub.execute_input":"2025-04-15T16:11:25.052947Z","iopub.status.idle":"2025-04-15T16:11:25.067614Z","shell.execute_reply.started":"2025-04-15T16:11:25.052926Z","shell.execute_reply":"2025-04-15T16:11:25.067077Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def run_task1():\n    \n    print(\"=== Running Task 1 (BiLSTM) ===\")\n    \n    def load_and_combine_data():\n        \n        train_dfs = []\n        for lang in ['en', 'hi', 'ta']:\n            df = load_data(lang, \"l1\", is_train=True)\n            train_dfs.append(df[['text', 'label']])\n        train_df = pd.concat(train_dfs, ignore_index=True)\n        return train_df['text'].tolist(), train_df['label'].tolist()\n    \n    def create_dataloaders(texts, labels):\n        \n        vocab = build_vocab(texts)\n        train_texts, val_texts, train_labels, val_labels = train_test_split(\n            texts, labels, test_size=0.2, random_state=42\n        )\n        \n        train_dataset = TextDataset(train_texts, vocab, train_labels)\n        val_dataset = TextDataset(val_texts, vocab, val_labels)\n        \n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n        return train_loader, val_loader, vocab\n    \n    def train_model(train_loader, val_loader, vocab):\n        \n        model = BiLSTMModel(\n            vocab_size=len(vocab),\n            embedding_dim=300,\n            hidden_dim=256,\n            output_dim=2,\n            n_layers=2,\n            dropout=0.5\n        )\n        return train_single_task(model, train_loader, val_loader, epochs=5)\n\n    # Main execution flow\n    texts, labels = load_and_combine_data()\n    train_loader, val_loader, vocab = create_dataloaders(texts, labels)\n    model, history = train_model(train_loader, val_loader, vocab)\n    return model, vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.068472Z","iopub.execute_input":"2025-04-15T16:11:25.068632Z","iopub.status.idle":"2025-04-15T16:11:25.083848Z","shell.execute_reply.started":"2025-04-15T16:11:25.068620Z","shell.execute_reply":"2025-04-15T16:11:25.083160Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def run_task2():\n    \n    print(\"\\n=== Running Task 2 (BiLSTM Transfer Learning) ===\")\n    \n    def load_hate_speech_data():\n        \n        def load_twitter_data():\n            \n            try:\n                from datasets import load_dataset\n                twitter_hate = load_dataset(\"tweets_hate_speech_detection\")\n                df = pd.DataFrame({\n                    'text': twitter_hate['train']['tweet'],\n                    'label': twitter_hate['train']['label']\n                })\n                print(f\"Loaded {len(df)} English hate speech samples from Twitter\")\n                return df\n            except Exception as e:\n                print(f\"Couldn't load Twitter hate speech data: {e}\")\n                return pd.DataFrame({\n                    'text': [\"you are stupid\", \"women are inferior\", \"this is normal\"],\n                    'label': [1, 1, 0]\n                })\n\n        def load_indic_data():\n            \n            hindi_df = pd.DataFrame({'text': [], 'label': []})\n            tamil_df = pd.DataFrame({'text': [], 'label': []})\n            \n            try:\n                hindi_df = pd.read_csv(\"/kaggle/input/macd-data/hindi_train.csv\")[['text', 'label']]\n                hindi_df['text'] = hindi_df['text'].apply(convert_hinglish_to_hindi)\n                print(f\"Loaded {len(hindi_df)} Hindi hate speech samples\")\n            except Exception as e:\n                print(f\"Couldn't load Hindi data: {e}\")\n                hindi_df = pd.DataFrame({\n                    'text': [\"तुम मूर्ख हो\", \"स्त्रियाँ अयोग्य हैं\", \"यह सामान्य है\"],\n                    'label': [1, 1, 0]\n                })\n\n            try:\n                tamil_df = pd.read_csv(\"/kaggle/input/macd-data/tamil_train.csv\")[['text', 'label']]\n                print(f\"Loaded {len(tamil_df)} Tamil hate speech samples\")\n            except Exception as e:\n                print(f\"Couldn't load Tamil data: {e}\")\n                tamil_df = pd.DataFrame({\n                    'text': [\"நீ முட்டாள்\", \"பெண்கள் தகுதியற்றவர்கள்\", \"இது சாதாரணமானது\"],\n                    'label': [1, 1, 0]\n                })\n            \n            return pd.concat([hindi_df, tamil_df], ignore_index=True)\n\n        english_hate = load_twitter_data()\n        indic_hate = load_indic_data()\n        return pd.concat([english_hate, indic_hate], ignore_index=True)\n\n    def pretrain_model(external_data):\n        \n        vocab_ext = build_vocab(external_data['text'])\n        train_dataset = TextDataset(external_data['text'].tolist(), vocab_ext, external_data['label'].tolist())\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n        \n        pretrain_model = BiLSTMModel(\n            vocab_size=len(vocab_ext),\n            embedding_dim=300,\n            hidden_dim=256,\n            output_dim=2,\n            n_layers=2,\n            dropout=0.5\n        )\n        \n        print(\"\\nPretraining on hate speech data...\")\n        return train_single_task(pretrain_model, train_loader, train_loader, epochs=3)\n\n    def prepare_task1_data():\n        \n        task1_texts, task1_labels = [], []\n        for lang in ['en', 'hi', 'ta']:\n            df = load_data(lang, \"l1\", is_train=True)\n            task1_texts.extend(df['text'].tolist())\n            task1_labels.extend(df['label'].tolist())\n        \n        vocab_task1 = build_vocab(task1_texts)\n        train_texts, val_texts, train_labels, val_labels = train_test_split(\n            task1_texts, task1_labels, test_size=0.2, random_state=42\n        )\n        \n        return vocab_task1, train_texts, val_texts, train_labels, val_labels\n\n    def create_fine_tune_model(pretrained_model, vocab_task1, task1_data):\n        \n        train_texts, val_texts, train_labels, val_labels = task1_data\n        \n        train_dataset = TextDataset(train_texts, vocab_task1, train_labels)\n        val_dataset = TextDataset(val_texts, vocab_task1, val_labels)\n        \n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n        \n        task2_model = BiLSTMModel(\n            vocab_size=len(vocab_task1),\n            embedding_dim=300,\n            hidden_dim=256,\n            output_dim=2,\n            n_layers=2,\n            dropout=0.5\n        )\n        \n        # Copy weights from pretrained model\n        task2_model.lstm.load_state_dict(pretrained_model.lstm.state_dict())\n        task2_model.fc.load_state_dict(pretrained_model.fc.state_dict())\n        \n        return task2_model, train_loader, val_loader\n\n    # Main execution flow\n    external_data = load_hate_speech_data()\n    pretrained_model, _ = pretrain_model(external_data)\n    \n    vocab_task1, *task1_data = prepare_task1_data()\n    task2_model, train_loader, val_loader = create_fine_tune_model(pretrained_model, vocab_task1, task1_data)\n    \n    print(\"\\nFine-tuning on Task 1 data...\")\n    final_model, history = train_single_task(task2_model, train_loader, val_loader, epochs=3)\n    \n    return final_model, vocab_task1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.084772Z","iopub.execute_input":"2025-04-15T16:11:25.085013Z","iopub.status.idle":"2025-04-15T16:11:25.101310Z","shell.execute_reply.started":"2025-04-15T16:11:25.084991Z","shell.execute_reply":"2025-04-15T16:11:25.100491Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def run_task3():\n    \n    print(\"\\n=== Running Task 3 (BiLSTM) ===\")\n    \n    def load_and_merge_data():\n        \n        train_dfs = []\n        for lang in ['en', 'hi', 'ta']:\n            df_l1 = load_data(lang, \"l1\", is_train=True)\n            df_l3 = load_data(lang, \"l3\", is_train=True)\n            merged = pd.merge(\n                df_l1[['text', 'label']].rename(columns={'label': 'label1'}),\n                df_l3[['text', 'label']].rename(columns={'label': 'label3'}),\n                on='text',\n                how='inner'\n            )\n            train_dfs.append(merged)\n        return pd.concat(train_dfs, ignore_index=True)\n\n    def prepare_data(train_df):\n        \n        texts = train_df['text'].tolist()\n        labels1 = train_df['label1'].tolist()\n        labels3 = train_df['label3'].tolist()\n        vocab = build_vocab(texts)\n        return texts, labels1, labels3, vocab\n\n    def split_data(texts, labels1, labels3):\n        \n        return train_test_split(\n            texts, labels1, labels3,\n            test_size=0.2,\n            random_state=42\n        )\n\n    def create_dataloaders(splits, vocab):\n        \n        train_texts, val_texts, train_labels1, val_labels1, train_labels3, val_labels3 = splits\n        train_dataset = TextDataset(train_texts, vocab, train_labels1, train_labels3)\n        val_dataset = TextDataset(val_texts, vocab, val_labels1, val_labels3)\n        \n        return (\n            DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn),\n            DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n        )\n\n    def train_model(train_loader, val_loader, vocab):\n        \n        model = MultiTaskBiLSTM(\n            vocab_size=len(vocab),\n            embedding_dim=300,\n            hidden_dim=256,\n            output_dim=2,\n            n_layers=2,\n            dropout=0.5\n        )\n        return train_multi_task(model, train_loader, val_loader, epochs=5)\n\n    # Main execution flow\n    train_df = load_and_merge_data()\n    texts, labels1, labels3, vocab = prepare_data(train_df)\n    splits = split_data(texts, labels1, labels3)\n    train_loader, val_loader = create_dataloaders(splits, vocab)\n    model, history = train_model(train_loader, val_loader, vocab)\n    \n    return model, vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.103313Z","iopub.execute_input":"2025-04-15T16:11:25.103526Z","iopub.status.idle":"2025-04-15T16:11:25.116648Z","shell.execute_reply.started":"2025-04-15T16:11:25.103512Z","shell.execute_reply":"2025-04-15T16:11:25.116061Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def evaluate(model, vocab, lang: str, label_type: str, is_multi_task: bool = False, head_idx: int = 0) -> float:\n    \n    \n    def prepare_data():\n        \n        df = load_data(lang, label_type, is_train=False)\n        if len(df) == 0 or 'text' not in df.columns or 'label' not in df.columns:\n            print(f\"No valid test data for {lang} {label_type}\")\n            return None\n            \n        df['text'] = df['text'].fillna('').astype(str)\n        df = df[df['text'].str.strip() != '']\n        return df if len(df) > 0 else None\n    \n    def create_loader(df):\n        \n        try:\n            dataset = TextDataset(df['text'].tolist(), vocab, df['label'].tolist())\n            return DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n        except Exception as e:\n            print(f\"Error creating dataset for {lang} {label_type}: {e}\")\n            return None\n            \n    def evaluate_multi_task(loader, device):\n        \n        model.eval()\n        preds, true_labels = [], []\n        \n        with torch.no_grad():\n            for batch in loader:\n                texts = batch['text'].to(device)\n                lengths = batch['lengths'].to(device)\n                labels = batch['label'].to(device)\n                \n                predictions1, predictions2 = model(texts, lengths)\n                predictions = predictions1 if head_idx == 0 else predictions2\n                batch_preds = torch.argmax(predictions, dim=-1)\n                \n                preds.extend(batch_preds.cpu().numpy())\n                true_labels.extend(labels.cpu().numpy())\n        \n        return f1_score(true_labels, preds, average='macro') if len(true_labels) > 0 else 0.0\n    \n    def evaluate_single_taskkk(loader, device):\n        \n        try:\n            f1, _ = evaluate_single_task(model, loader, device)\n            return f1\n        except Exception as e:\n            print(f\"Error during evaluation for {lang} {label_type}: {e}\")\n            return 0.0\n    \n    # Main execution flow\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    df = prepare_data()\n    if df is None:\n        return 0.0\n        \n    loader = create_loader(df)\n    if loader is None:\n        return 0.0\n        \n    return evaluate_multi_task(loader, device) if is_multi_task else evaluate_single_taskkk(loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.117361Z","iopub.execute_input":"2025-04-15T16:11:25.117547Z","iopub.status.idle":"2025-04-15T16:11:25.132708Z","shell.execute_reply.started":"2025-04-15T16:11:25.117533Z","shell.execute_reply":"2025-04-15T16:11:25.132020Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\n\n\n\ndef infer_task1(text: str, model, vocab) -> Dict[str, float]:\n    \n    def prepare_input():\n        \n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model.to(device)\n        indices = [vocab.word2idx.get(word.lower(), vocab.word2idx['<unk>']) for word in text.split()]\n        tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n        length = torch.LongTensor([len(indices)]).to(device)\n        return tensor, length, device\n\n    def get_prediction(tensor, length):\n        \n        with torch.no_grad():\n            output = model(tensor, length)\n            probs = torch.softmax(output, dim=-1).cpu().numpy()[0]\n            pred = np.argmax(probs)\n            return pred, float(probs[pred])\n\n    # Main execution flow\n    tensor, length, _ = prepare_input()\n    pred, confidence = get_prediction(tensor, length)\n    \n    return {\n        \"label\": \"Gendered Abuse\" if pred == 1 else \"Not Gendered Abuse\",\n        \"confidence\": confidence,\n        \"task\": \"Task 1 (Original)\"\n    }\n\ndef infer_task3(text: str, model, vocab) -> Dict[str, Dict[str, float]]:\n    \n    def prepare_input():\n        \n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model.to(device)\n        indices = [vocab.word2idx.get(word.lower(), vocab.word2idx['<unk>']) for word in text.split()]\n        tensor = torch.LongTensor(indices).unsqueeze(0).to(device) \n        length = torch.LongTensor([len(indices)]).to(device)\n        return tensor, length\n\n    def get_predictions(tensor, length):\n        \n        with torch.no_grad():\n            output1, output2 = model(tensor, length)\n            probs1 = torch.softmax(output1, dim=-1).cpu().numpy()[0]\n            probs2 = torch.softmax(output2, dim=-1).cpu().numpy()[0]\n            \n        return {\n            \"Label l1 (Gendered Abuse)\": {\n                \"prediction\": \"Yes\" if np.argmax(probs1) == 1 else \"No\",\n                \"confidence\": float(probs1[np.argmax(probs1)])\n            },\n            \"Label l3 (Explicit)\": {\n                \"prediction\": \"Yes\" if np.argmax(probs2) == 1 else \"No\", \n                \"confidence\": float(probs2[np.argmax(probs2)])\n            }\n        }\n\n    # Main execution flow\n    tensor, length = prepare_input()\n    return get_predictions(tensor, length)\n\n# ======== Main Execution ============\nif __name__ == \"__main__\":\n    def train_all_models():\n        \n        print(\"=== Training Task 1 ===\")\n        model1, vocab1 = run_task1()\n        \n        print(\"\\n=== Training Task 2 ===\")\n        model2, vocab2 = run_task2()\n        \n        print(\"\\n=== Training Task 3 ===\")\n        model3, vocab3 = run_task3()\n        \n        return (model1, vocab1), (model2, vocab2), (model3, vocab3)\n    \n    def evaluate_and_test_models(models_and_vocabs):\n        \n        (model1, vocab1), (model2, vocab2), (model3, vocab3) = models_and_vocabs\n        \n        print(\"\\n=== Evaluation ===\")\n        for lang in ['en', 'hi', 'ta']:\n            # Task 1/2 (Label l1)\n            task1_f1 = evaluate(model1, vocab1, lang, \"l1\")\n            task2_f1 = evaluate(model2, vocab2, lang, \"l1\")\n            # Task 3 (Labels l1 and l3)\n            task3_l1_f1 = evaluate(model3, vocab3, lang, \"l1\", is_multi_task=True, head_idx=0)\n            task3_l3_f1 = evaluate(model3, vocab3, lang, \"l3\", is_multi_task=True, head_idx=1)\n            \n            print(f\"\\n{lang.upper()} Results:\")\n            print(f\"Task 1 (l1): {task1_f1:.4f} | Task 2 (l1): {task2_f1:.4f}\")\n            print(f\"Task 3 (l1): {task3_l1_f1:.4f} | Task 3 (l3): {task3_l3_f1:.4f}\")\n        \n        # Interactive inference\n        test_text = \"Women belong in the kitchen\"\n        print(\"\\n=== Predictions ===\")\n        print(\"Task 1:\", infer_task1(test_text, model1, vocab1))\n        print(\"Task 3:\", infer_task3(test_text, model3, vocab3))\n\n    # Execute the nested functions\n    models_and_vocabs = train_all_models()\n    evaluate_and_test_models(models_and_vocabs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:11:25.133514Z","iopub.execute_input":"2025-04-15T16:11:25.133720Z","iopub.status.idle":"2025-04-15T16:15:49.541842Z","shell.execute_reply.started":"2025-04-15T16:11:25.133699Z","shell.execute_reply":"2025-04-15T16:15:49.540997Z"}},"outputs":[{"name":"stdout","text":"=== Training Task 1 ===\n=== Running Task 1 (BiLSTM) ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2763cf9711e14835afbf08ca68ae26a7"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.5912 | Val Loss: 0.5475 | Val F1: 0.6421\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0058bf05e4d24230ac08a6520e321466"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.5249 | Val Loss: 0.5245 | Val F1: 0.6919\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9935bdaf74634f47ad70523d7507e99b"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.4716 | Val Loss: 0.5334 | Val F1: 0.7062\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"287ad9b310af4398b789c06cd3aeb414"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.4204 | Val Loss: 0.5604 | Val F1: 0.7119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34202ecd0aef407394087721c882a052"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.3686 | Val Loss: 0.6016 | Val F1: 0.7152\n\n=== Training Task 2 ===\n\n=== Running Task 2 (BiLSTM Transfer Learning) ===\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3322945942.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded 31962 English hate speech samples from Twitter\nLoaded 20183 Hindi hate speech samples\nLoaded 18000 Tamil hate speech samples\n\nPretraining on hate speech data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/2193 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f975656fb70147ed83c7bec2c4979d7a"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.4108 | Val Loss: 0.2976 | Val F1: 0.8525\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/2193 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3768d59c604b5a89fbfbf370fedb40"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.3070 | Val Loss: 0.2124 | Val F1: 0.8936\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/2193 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d8a7169b11a4494a5ccef2c7fcfd375"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.2601 | Val Loss: 0.1707 | Val F1: 0.9179\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3322945942.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"\nFine-tuning on Task 1 data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792eb59142284c37aefad0a1941b5dfb"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.6137 | Val Loss: 0.5749 | Val F1: 0.6006\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a91f96f7a52d4db0927c19882a3036af"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.5571 | Val Loss: 0.5583 | Val F1: 0.6805\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c3011a02d5450d9cd67669171366f2"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.5051 | Val Loss: 0.5429 | Val F1: 0.6962\n\n=== Training Task 3 ===\n\n=== Running Task 3 (BiLSTM) ===\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3322945942.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pt'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d850a59a734e49be57e61b0ffe3957"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Train Loss: 1.2145 | Val Loss: 0.5619\nL1: F1=0.6810 | L3: F1=0.6775\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6faf9883c14dbd90ecc96ce4ecc8ed"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Train Loss: 1.0888 | Val Loss: 0.5416\nL1: F1=0.6937 | L3: F1=0.7043\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92bdfd7c89ca475c9c711985e607fcfc"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.9975 | Val Loss: 0.5512\nL1: F1=0.6848 | L3: F1=0.7111\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"011386a622bd4bc3b80c40653d114a08"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.9136 | Val Loss: 0.5648\nL1: F1=0.6941 | L3: F1=0.7245\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b27f0227ad94e94b6d84ae550444663"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.8357 | Val Loss: 0.5703\nL1: F1=0.6844 | L3: F1=0.7268\n\n=== Evaluation ===\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/447767804.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_multi_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"\nEN Results:\nTask 1 (l1): 0.6191 | Task 2 (l1): 0.5909\nTask 3 (l1): 0.5836 | Task 3 (l3): 0.5591\n\nHI Results:\nTask 1 (l1): 0.6365 | Task 2 (l1): 0.5925\nTask 3 (l1): 0.6275 | Task 3 (l3): 0.7025\n\nTA Results:\nTask 1 (l1): 0.7637 | Task 2 (l1): 0.7627\nTask 3 (l1): 0.7753 | Task 3 (l3): 0.8538\n\n=== Predictions ===\nTask 1: {'label': 'Gendered Abuse', 'confidence': 0.757101833820343, 'task': 'Task 1 (Original)'}\nTask 3: {'Label l1 (Gendered Abuse)': {'prediction': 'Yes', 'confidence': 0.773901641368866}, 'Label l3 (Explicit)': {'prediction': 'Yes', 'confidence': 0.8987719416618347}}\n","output_type":"stream"}],"execution_count":29}]}